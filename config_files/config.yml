# The base configuration of the benchmark

# Enable logging
log: True
# The name of each dataset in the directory on which the benchmark should be run
name: ["plausible"]
# A label for the resul directory
label: "_"
# The type of dataset, currently supported ".hdf5", and ".csv"
file_type: ".hdf5"
# The views to use in the banchmark, an empty value will result in using all the views
views:
# The path to the directory where the datasets are stored
pathf: "../data/"
# The niceness of the processes, useful to lower their priority
nice: 0
# The random state of the benchmark, useful for reproducibility
random_state: 42
# The number of parallel computing threads
nb_cores: 1
# Used to run the benchmark on the full dataset
full: False
# Used to be able to run more than one benchmark per minute
debug: False
# To add noise to the data, will add gaussian noise with noise_std
add_noise: False
noise_std: 0.0
# The directory in which the results will be stored
res_dir: "../results/"
# If an error occurs in a classifier, if track_tracebacks is set to True, the
# benchmark saves the traceback and continues, if it is set to False, it will
# stop the benchmark and raise the error
track_tracebacks: True

# If the dataset is multiclass, will use this multiclass-to-biclass method
multiclass_method: "oneVersusOne"
# The ratio number of test exmaples/number of train examples
split: 0.8
# The nubmer of folds in the cross validation process when hyper-paramter optimization is performed
nb_folds: 2
# The number of classes to select in the dataset
nb_class: 2
# The name of the classes to select in the dataset
classes:
# The type of algorithms to run during the benchmark (monoview and/or multiview)
type: ["monoview","multiview"]
# The name of the monoview algorithms to run, ["all"] to run all the available classifiers
algos_monoview: ["all"]
# The names of the multiview algorithms to run, ["all"] to run all the available classifiers
algos_multiview: ["all"]
# The number of times the benchamrk is repeated with different train/test
# split, to have more statistically significant results
stats_iter: 1
# The metrics that will be use din the result analysis
metrics: ["accuracy_score", "f1_score"]
# The metric that will be used in the hyper-parameter optimization process
metric_princ: "f1_score"
# The type of hyper-parameter optimization method
hps_type: "randomized_search"
# The number of iteration in the hyper-parameter optimization process
hps_iter: 2


# The following arguments are classifier-specific, and are documented in each
# of the corresponding modules.

# In order to run multiple sets of parameters, use multiple values in the
# following lists, and set hps_type to None.

#####################################
# The Monoview Classifier arguments #
#####################################


random_forest:
  n_estimators: [25]
  max_depth: [3]
  criterion: ["entropy"]

svm_linear:
  C: [1]

svm_rbf:
  C: [1]

svm_poly:
  C: [1]
  degree: [2]

adaboost:
  n_estimators: [50]
  base_estimator: ["DecisionTreeClassifier"]

adaboost_pregen:
  n_estimators: [50]
  base_estimator: ["DecisionTreeClassifier"]
  n_stumps: [1]

adaboost_graalpy:
  n_iterations: [50]
  n_stumps: [1]

decision_tree:
  max_depth: [10]
  criterion: ["gini"]
  splitter: ["best"]

decision_tree_pregen:
  max_depth: [10]
  criterion: ["gini"]
  splitter: ["best"]
  n_stumps: [1]

sgd:
  loss: ["hinge"]
  penalty: [l2]
  alpha: [0.0001]

knn:
  n_neighbors: [5]
  weights: ["uniform"]
  algorithm: ["auto"]

scm:
  model_type: ["conjunction"]
  max_rules: [10]
  p: [0.1]

scm_pregen:
  model_type: ["conjunction"]
  max_rules: [10]
  p: [0.1]
  n_stumps: [1]

cq_boost:
  mu: [0.01]
  epsilon: [1e-06]
  n_max_iterations: [5]
  n_stumps: [1]

cg_desc:
  n_max_iterations: [10]
  n_stumps: [1]

cb_boost:
  n_max_iterations: [10]
  n_stumps: [1]

lasso:
  alpha: [1]
  max_iter: [2]

gradient_boosting:
  n_estimators: [2]


######################################
# The Multiview Classifier arguments #
######################################

weighted_linear_early_fusion:
  view_weights: [null]
  monoview_classifier_name: ["decision_tree"]
  monoview_classifier_config:
    decision_tree:
      max_depth: [1]
      criterion: ["gini"]
      splitter: ["best"]

entropy_fusion:
  classifier_names: [["decision_tree"]]
  classifier_configs:
    decision_tree:
      max_depth: [1]
      criterion: ["gini"]
      splitter: ["best"]

disagree_fusion:
  classifier_names: [["decision_tree"]]
  classifier_configs:
    decision_tree:
      max_depth: [1]
      criterion: ["gini"]
      splitter: ["best"]


double_fault_fusion:
  classifier_names: [["decision_tree"]]
  classifier_configs:
    decision_tree:
      max_depth: [1]
      criterion: ["gini"]
      splitter: ["best"]

difficulty_fusion:
  classifier_names: [["decision_tree"]]
  classifier_configs:
    decision_tree:
      max_depth: [1]
      criterion: ["gini"]
      splitter: ["best"]

scm_late_fusion:
  classifier_names: [["decision_tree"]]
  p: 0.1
  max_rules: 10
  model_type: 'conjunction'
  classifier_configs:
    decision_tree:
      max_depth: [1]
      criterion: ["gini"]
      splitter: ["best"]

majority_voting_fusion:
  classifier_names: [["decision_tree", "decision_tree", "decision_tree", ]]
  classifier_configs:
    decision_tree:
      max_depth: [1]
      criterion: ["gini"]
      splitter: ["best"]

bayesian_inference_fusion:
  classifier_names: [["decision_tree", "decision_tree", "decision_tree", ]]
  classifier_configs:
    decision_tree:
      max_depth: [1]
      criterion: ["gini"]
      splitter: ["best"]

weighted_linear_late_fusion:
  classifier_names: [["decision_tree", "decision_tree", "decision_tree", ]]
  classifier_configs:
    decision_tree:
      max_depth: [1]
      criterion: ["gini"]
      splitter: ["best"]

mumbo:
  base_estimator: [null]
  n_estimators: [10]
  best_view_mode: ["edge"]